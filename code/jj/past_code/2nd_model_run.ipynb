{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f533e493-4094-40a2-954d-16b49c27acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5e80ad-1951-48aa-8844-25cf7cb93fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de58e13-a279-4552-a56e-c28f945e7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/data/train/images_classified/'\n",
    "dataset = datasets.ImageFolder(os.path.join(data_dir), transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c3b2df-14c7-4e8b-8a44-2497b3c553fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 18900\n",
       "    Root location: ../input/data/train/images_classified/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b694196-2bbb-4c31-8d1a-05767d3ddbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "class_names = dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea09489-3b69-4163-a7b3-2f5541844a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e7459c-7239-4adf-9a85-b92f0b802f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa759912-93e5-4ceb-8117-038152b26dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=8)\n",
    "dataloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a1b4cf-d702-4597-baaa-9244eda02ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 학습 데이터를 배치 단위로 불러오기\\niterator = iter(dataloader)\\n\\n# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\\ninputs, classes= next(iterator)\\nout = torchvision.utils.make_grid(inputs)\\nimshow(out, title=[class_names[x] for x in classes])\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(input, title):\n",
    "    # torch.Tensor를 numpy 객체로 변환\n",
    "    # pytorch는 batch, channel, 높이, 너비 순으로 데이터 구성.\n",
    "    input = input.numpy().transpose((1, 2, 0)) # channel 값이 가장 뒤에 오도록.\n",
    "    # 이미지 정규화 해제하기\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # 이미지 출력\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "# 학습 데이터를 배치 단위로 불러오기\n",
    "iterator = iter(dataloader)\n",
    "\n",
    "# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\n",
    "inputs, classes= next(iterator)\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d819e4e-4e5e-476d-a0d0-37b662af0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 18개로 교체하여 마지막 레이어 다시 학습\n",
    "model.fc = nn.Linear(num_features, 18) \n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2d156e-e689-48bc-a8da-214fde3ee227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:22<00:00,  8.26s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 1.2791 Acc: 63.5185% Time: 982.5978s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:20<00:00,  8.24s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 Loss: 0.4315 Acc: 87.5000% Time: 1963.0354s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:24<00:00,  8.27s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#2 Loss: 0.2760 Acc: 91.2963% Time: 2947.2464s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:22<00:00,  8.26s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#3 Loss: 0.1993 Acc: 93.7897% Time: 3929.6649s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:18<00:00,  8.22s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#4 Loss: 0.1449 Acc: 95.7474% Time: 4908.1767s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:17<00:00,  8.21s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5 Loss: 0.1087 Acc: 97.0172% Time: 5885.6935s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:21<00:00,  8.25s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#6 Loss: 0.0814 Acc: 97.8968% Time: 6867.0584s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:20<00:00,  8.24s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#7 Loss: 0.0631 Acc: 98.6905% Time: 7847.4666s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:16<00:00,  8.20s/it]\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#8 Loss: 0.0457 Acc: 99.1336% Time: 8823.8568s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [16:20<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#9 Loss: 0.0347 Acc: 99.4312% Time: 9804.3057s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#num_epochs = 50\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "# 전체 반복(epoch) 수 만큼 반복하며\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    # 배치 단위로 학습 데이터 불러오기\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 모델에 입력(forward)하고 결과 계산\n",
    "        optimizer.zero_grad() # 전체 grad 값을 초기화.\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_set)\n",
    "    epoch_acc = running_corrects / len(train_set) * 100.\n",
    "\n",
    "    # 학습 과정 중에 결과 출력\n",
    "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104a6709-fe12-4dd4-af87-777e21815a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c9bee9-ee14-45fe-9d18-116e121006e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bfd2352ec9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "start_time = time.time()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in valid_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        #all_preds.append(preds)\n",
    "        \n",
    "        for p in preds.numpy():\n",
    "            all_preds.append(p)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        \n",
    "        #all_labels.append(labels.data)\n",
    "        \n",
    "        for l in labels.data.numpy():\n",
    "            all_labels.append(l)\n",
    "        \n",
    "        # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
    "        print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
    "        imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_set)\n",
    "    epoch_acc = running_corrects / len(valid_set) * 100.\n",
    "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a2942ce-61ad-44a7-9fb2-48a13a9d8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9e16714-7ee8-4770-ab94-9bd5555d30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e30ea202-b564-4a26-9a5c-78bfd77bf3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge torchmetrics -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "deb089a9-1b52-42c1-9220-f5a8a4e5aed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.871437293012491"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#from torchmetrics import F1Score\n",
    "\n",
    "#f1 = F1Score(num_classes=18)\n",
    "#f1(all_preds, all_labels)\n",
    "f1_score(all_labels, all_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34734f3c-31ea-4cf0-b52e-d89afede2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f39cd5ce-5655-4244-946c-2eb4d8614bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9228d234-7987-47a0-b169-3dc4e3c56978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>d71d4570505d6af8f777690e63edfa8d85ea4476.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>6cf1300e8e218716728d5820c0bab553306c2cfd.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>8140edbba31c3a824e817e6d5fb95343199e2387.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ImageID  ans\n",
       "0      cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1      0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2      b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3      4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4      248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0\n",
       "...                                             ...  ...\n",
       "12595  d71d4570505d6af8f777690e63edfa8d85ea4476.jpg    0\n",
       "12596  6cf1300e8e218716728d5820c0bab553306c2cfd.jpg    0\n",
       "12597  8140edbba31c3a824e817e6d5fb95343199e2387.jpg    0\n",
       "12598  030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg    0\n",
       "12599  f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg    0\n",
       "\n",
       "[12600 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a72bc02b-2c41-41f3-bb33-bcbbddea900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3d39312-6bb2-43b6-9a9b-a0b9491e66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9bbda1a-9992-450a-9ad0-5eeb7466e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f550a068-2ab4-4c96-9d59-9446aaee5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TestDataset(image_paths, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21810ff5-4600-4fd2-acc3-476ec461907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2018d053-589a-4832-931d-40a6314dde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_test(input):\n",
    "    # torch.Tensor를 numpy 객체로 변환\n",
    "    # pytorch는 batch, channel, 높이, 너비 순으로 데이터 구성.\n",
    "    input = input.numpy().transpose((1, 2, 0)) # channel 값이 가장 뒤에 오도록.\n",
    "    # 이미지 정규화 해제하기\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # 이미지 출력\n",
    "    plt.imshow(input)\n",
    "    #plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "35bb15ec-d892-426c-b270-dac6058c1c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 데이터를 배치 단위로 불러오기\\niterator = iter(test_dataloader)\\n\\n# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\\ninputs = next(iterator)\\nout = torchvision.utils.make_grid(inputs)\\nimshow_test(out)\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 데이터를 배치 단위로 불러오기\n",
    "iterator = iter(test_dataloader)\n",
    "\n",
    "# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\n",
    "inputs = next(iterator)\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow_test(out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1918ad07-beb1-426e-8ae3-ea5e0718d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [04:56<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(test_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cb992f19-c937-4ee9-8a9e-caad5936ba58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\n\\nall_predictions = []\\nfor images in tqdm(test_dataloader):\\n    with torch.no_grad():\\n        images = images.to(device)\\n        pred = model(images)\\n        pred = pred.argmax(dim=-1)\\n        all_predictions.extend(pred.cpu().numpy())\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from PIL import Image\n",
    "\n",
    "all_predictions = []\n",
    "for images in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e905798a-278f-46bd-ba56-ac3f757beccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1 5 5 4 \n",
      "0 15 11 13 11 \n",
      "5 11 14 11 0 \n",
      "12 1 1 15 1 \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_predictions)):\n",
    "    print(all_predictions[i], end=' ')\n",
    "    \n",
    "    if i % 5 == 4:\n",
    "        print()\n",
    "    \n",
    "    if i == 19:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74897034-f975-486e-954c-b5457ec2cbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dee7ae77-5091-4b46-aeaa-4ee6567410b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions2 = []\n",
    "\n",
    "for p in all_predictions:\n",
    "    all_predictions2.append(class_names[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ddbba00-651c-4a12-9293-fd0eed9e90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1 13 13 12 \n",
      "0 7 3 5 3 \n",
      "13 3 6 3 0 \n",
      "4 1 1 7 1 \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_predictions2)):\n",
    "    print(all_predictions2[i], end=' ')\n",
    "    \n",
    "    if i % 5 == 4:\n",
    "        print()\n",
    "    \n",
    "    if i == 19:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ebd93a3-1833-42a9-a7c7-866a8c0d3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['ans'] = all_predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0eb4ec12-2390-458a-96f2-16a39541c767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>d71d4570505d6af8f777690e63edfa8d85ea4476.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>6cf1300e8e218716728d5820c0bab553306c2cfd.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>8140edbba31c3a824e817e6d5fb95343199e2387.jpg</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ImageID ans\n",
       "0      cbc5c6e168e63498590db46022617123f1fe1268.jpg  13\n",
       "1      0e72482bf56b3581c081f7da2a6180b8792c7089.jpg   1\n",
       "2      b549040c49190cedc41327748aeb197c1670f14d.jpg  13\n",
       "3      4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg  13\n",
       "4      248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg  12\n",
       "...                                             ...  ..\n",
       "12595  d71d4570505d6af8f777690e63edfa8d85ea4476.jpg   1\n",
       "12596  6cf1300e8e218716728d5820c0bab553306c2cfd.jpg   4\n",
       "12597  8140edbba31c3a824e817e6d5fb95343199e2387.jpg   9\n",
       "12598  030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg   1\n",
       "12599  f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg   7\n",
       "\n",
       "[12600 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17b4762c-bf6e-4052-b55d-bad604e3e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "325b6364-e150-4064-b2e2-b85df4a00639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "801dd4f6-fe92-4137-bac8-cd4d5fa26f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/code'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "92660475-19fe-49ab-aa65-8722601b7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.getcwd()+'/'+'2nd_model_run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569694c1-b426-43b3-8eff-ff3f5c99f49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
